{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST data is split into three parts: 55,000 data points of training data (mnist.train), 1000 points of test data (mnist.test), and 5000 points of validation data (mnist.validation). MNIST data point has two parts. mnist.train.images correspond to mnist.train.labels. \n",
    "\n",
    "We flatten the 28x28 matrix into 784 numbers as softmax/logistic regression won't care about the structure. The result is that mnist.train.images is a tensor (an n-dimensional array) with a shape of [55000, 784]. Each entry in the tensor is a pixel intensity between 0 and 1.\n",
    "\n",
    "Each image in MNIST has a corresponding label, a number between 0 and 9 representing the digit drawn in the image. \n",
    "\n",
    "We store the labels as \"one-hot vectors\" Which means something like 3 would be [0,0,0,1,0,0,0,0,0,0]. Conseuqnetly, mnist.train.labels is a [55000,10] array of floats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, we want to be able to say that some things are more likely independent of the input by adding a bias. The result is that the evidence for class i given an input x is\n",
    "\n",
    "evidence$_i$ = $\\sum_j W_{i, j}x_{j} + b_i$\n",
    "\n",
    "where $W_i$ is the weights and $b_i$ is the bias for class i, and j is an index for summing over the pixels in our input image x. We then convert the evidence tallies into our predicted probabilities y using the \"softmax\" function\n",
    "\n",
    "y = softmax(evidence)\n",
    "\n",
    "Here softmax is serving as an \"activatoin\" or \"link\" function, shaping the output of our linear functoin into the form we want - in this case, a probability distribution over 10 cases. You can think of it as converting tallies of evidence into probabilities of our input being in each class. It's defined as:\n",
    "\n",
    "softmax(x) = normalize(exp(x))\n",
    "\n",
    "And once expanded, you attain\n",
    "\n",
    "$softmax(x)_i = \\frac{exp(x_i)}{\\sum_j exp(x_j)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "#x isn't a specific value, it's a placeholder\n",
    "\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "#W is 784, 10 so when we multiply it by the 784,1 matrix, we create a \n",
    "#10x1 matrix of confidence intervals\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "In order to train our model, we need to define a cost function. One very common very nice function to determine the loss is called \"cross entropy\" Cross-entropy arises fro mthinking about information compressing codes in information theory but it winds up being an important idea in a lot of areas, from gambling to machine learning. It's defined as\n",
    "\n",
    "$H_{y'}(y) = -\\sum_i y_{i}^{'}log(y_i)$\n",
    "\n",
    "where y is our predicted probability distribution, and y' is the true distribution (the one-hot vector with digit labels.) In some rough sense, the cross-entropy is measuring how inefficient our predictions.\n",
    "\n",
    "To implement cross-entropy we need to first add a new placeholder to input correct answers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we implement cross entropy function\n",
    "\n",
    "$-\\sum y'log(y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices = [1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, tf.log computes the ogarithm of each element of y. Next, we multiply each element of y_ with the corresponding element of tf.log(y). Then tf.reduce_sum adds the elements in the second dimension of y, due to reduction_indices[1] parameter. Finally, tf.reduce_mean computes the mean over all the examles in the batch. \n",
    "\n",
    "Note that in the source code, we don't use this formulation, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_:batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
